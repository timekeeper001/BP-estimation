{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c99a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from LBFGS import FullBatchLBFGS\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6aeaff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning to run on 3 GPUs.\n"
     ]
    }
   ],
   "source": [
    "#data=pd.read_csv('QingData_dbp_19f.csv')\n",
    "data = pd.read_csv('data_47p_1h_21f.csv')\n",
    "#data=data.sample(frac=0.5,random_state=1)\n",
    "patient_list=data['patient_id'].unique()\n",
    "#print(patient_list)\n",
    "####################################SBP 加‘20’\n",
    "#feature=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']\n",
    "feature=['HrRR','HrSS','AmFS','AmSN','AmFN','AmFN_FS','AmFN_SN','TmFN','TmNF','TmFN_NF','Tm_FS','Tm_SF','Tm_SN','Tm_FQ','PAT_S','PAT_F','PAT_Q','ArFS','ArSN','ArNF','ArNF_FN']\n",
    "n_devices = 3#torch.cuda.device_count()\n",
    "print('Planning to run on {} GPUs.'.format(n_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67f7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L=len(patient_list)\n",
    "# for id in range(L):\n",
    "#     target_data=data[data['patient_id']==patient_list[id]]\n",
    "#     print(id,patient_list[id])\n",
    "#     target_data['SBP'].plot.hist(grid=True, bins=50,rwidth=0.9,color='#607c8e')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3749c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.01 GiB (GPU 0; 10.92 GiB total capacity; 107.89 MiB already allocated; 10.12 GiB free; 134.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62661\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.50 GiB (GPU 0; 10.92 GiB total capacity; 333.20 MiB already allocated; 9.80 GiB free; 352.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31331\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.75 GiB (GPU 0; 10.92 GiB total capacity; 10.00 GiB already allocated; 111.38 MiB free; 10.03 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15666\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.88 GiB (GPU 0; 10.92 GiB total capacity; 10.01 GiB already allocated; 107.38 MiB free; 10.03 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7833\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 7.67 GiB already allocated; 93.38 MiB free; 10.04 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 1; 10.92 GiB total capacity; 8.62 GiB already allocated; 363.38 MiB free; 9.83 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_qing_liu/Jiacheng /LBFGS.py:257: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554788289/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1 - Loss: 12.890   lengthscale: 39.797   noise: 3.541\n",
      "Finished training on 125321 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.890   lengthscale: 39.797   noise: 3.541\n",
      "Iter 2/100 - Loss: 12.174   lengthscale: 39.787   noise: 3.882\n",
      "Iter 3/100 - Loss: 11.412   lengthscale: 39.770   noise: 4.309\n",
      "Iter 4/100 - Loss: 10.737   lengthscale: 39.744   noise: 4.759\n",
      "Iter 5/100 - Loss: 10.117   lengthscale: 39.708   noise: 5.247\n",
      "Iter 6/100 - Loss: 9.551   lengthscale: 39.660   noise: 5.771\n",
      "Iter 7/100 - Loss: 9.029   lengthscale: 39.596   noise: 6.339\n",
      "Iter 8/100 - Loss: 8.548   lengthscale: 39.513   noise: 6.953\n",
      "Iter 9/100 - Loss: 8.103   lengthscale: 39.408   noise: 7.618\n",
      "Iter 10/100 - Loss: 7.687   lengthscale: 39.276   noise: 8.340\n",
      "Iter 11/100 - Loss: 7.304   lengthscale: 39.113   noise: 9.117\n",
      "Iter 12/100 - Loss: 6.949   lengthscale: 38.898   noise: 9.958\n",
      "Iter 13/100 - Loss: 6.621   lengthscale: 38.619   noise: 10.861\n",
      "Iter 14/100 - Loss: 6.316   lengthscale: 38.257   noise: 11.834\n",
      "Iter 15/100 - Loss: 6.035   lengthscale: 37.794   noise: 12.878\n",
      "Iter 16/100 - Loss: 5.777   lengthscale: 37.207   noise: 13.991\n",
      "Iter 17/100 - Loss: 5.538   lengthscale: 36.457   noise: 15.184\n",
      "Iter 18/100 - Loss: 5.317   lengthscale: 35.505   noise: 16.456\n",
      "Iter 19/100 - Loss: 5.112   lengthscale: 34.290   noise: 17.814\n",
      "Iter 20/100 - Loss: 4.921   lengthscale: 32.737   noise: 19.260\n",
      "Iter 21/100 - Loss: 4.740   lengthscale: 30.718   noise: 20.809\n",
      "Iter 22/100 - Loss: 4.562   lengthscale: 28.022   noise: 22.496\n",
      "Iter 23/100 - Loss: 4.379   lengthscale: 24.283   noise: 24.363\n",
      "Iter 24/100 - Loss: 4.160   lengthscale: 18.472   noise: 26.647\n",
      "Iter 25/100 - Loss: 3.609   lengthscale: 1.892   noise: 32.039\n",
      "Iter 26/100 - Loss: 3.573   lengthscale: 3.279   noise: 31.615\n",
      "Iter 27/100 - Loss: 3.572   lengthscale: 3.279   noise: 31.615\n",
      "Convergence reached!\n",
      "Finished training on 125321 data points using 3 GPUs.\n",
      "3100033 MAE:  11.518 ME 11.155 STD:  9.916\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 38.96 GiB (GPU 0; 10.92 GiB total capacity; 346.22 MiB already allocated; 9.17 GiB free; 982.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62625\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.48 GiB (GPU 0; 10.92 GiB total capacity; 569.86 MiB already allocated; 9.35 GiB free; 796.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31313\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.74 GiB (GPU 0; 10.92 GiB total capacity; 564.88 MiB already allocated; 9.35 GiB free; 796.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.87 GiB (GPU 0; 10.92 GiB total capacity; 5.35 GiB already allocated; 4.48 GiB free; 5.65 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7829\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 7.89 GiB already allocated; 2.04 GiB free; 8.09 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3915\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 1; 10.92 GiB total capacity; 8.63 GiB already allocated; 347.38 MiB free; 9.84 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1958\n",
      "Iter 1/1 - Loss: 12.451   lengthscale: 39.854   noise: 3.477\n",
      "Finished training on 125250 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.451   lengthscale: 39.854   noise: 3.477\n",
      "Iter 2/100 - Loss: 11.771   lengthscale: 39.845   noise: 3.808\n",
      "Iter 3/100 - Loss: 11.040   lengthscale: 39.827   noise: 4.229\n",
      "Iter 4/100 - Loss: 10.390   lengthscale: 39.803   noise: 4.674\n",
      "Iter 5/100 - Loss: 9.797   lengthscale: 39.768   noise: 5.153\n",
      "Iter 6/100 - Loss: 9.252   lengthscale: 39.721   noise: 5.671\n",
      "Iter 7/100 - Loss: 8.754   lengthscale: 39.658   noise: 6.229\n",
      "Iter 8/100 - Loss: 8.292   lengthscale: 39.577   noise: 6.834\n",
      "Iter 9/100 - Loss: 7.865   lengthscale: 39.475   noise: 7.487\n",
      "Iter 10/100 - Loss: 7.471   lengthscale: 39.346   noise: 8.194\n",
      "Iter 11/100 - Loss: 7.105   lengthscale: 39.187   noise: 8.959\n",
      "Iter 12/100 - Loss: 6.764   lengthscale: 38.977   noise: 9.785\n",
      "Iter 13/100 - Loss: 6.450   lengthscale: 38.703   noise: 10.673\n",
      "Iter 14/100 - Loss: 6.159   lengthscale: 38.350   noise: 11.629\n",
      "Iter 15/100 - Loss: 5.892   lengthscale: 37.900   noise: 12.650\n",
      "Iter 16/100 - Loss: 5.645   lengthscale: 37.325   noise: 13.743\n",
      "Iter 17/100 - Loss: 5.416   lengthscale: 36.593   noise: 14.914\n",
      "Iter 18/100 - Loss: 5.206   lengthscale: 35.668   noise: 16.156\n",
      "Iter 19/100 - Loss: 5.010   lengthscale: 34.486   noise: 17.485\n",
      "Iter 20/100 - Loss: 4.828   lengthscale: 32.972   noise: 18.905\n",
      "Iter 21/100 - Loss: 4.656   lengthscale: 31.019   noise: 20.416\n",
      "Iter 22/100 - Loss: 4.488   lengthscale: 28.425   noise: 22.050\n",
      "Iter 23/100 - Loss: 4.317   lengthscale: 24.852   noise: 23.851\n",
      "Iter 24/100 - Loss: 4.117   lengthscale: 19.393   noise: 26.018\n",
      "Iter 25/100 - Loss: 3.541   lengthscale: 2.828   noise: 31.375\n",
      "Iter 27/100 - Loss: 3.536   lengthscale: 3.413   noise: 31.226\n",
      "Convergence reached!\n",
      "Finished training on 125250 data points using 3 GPUs.\n",
      "3100119 MAE:  20.912 ME -20.796 STD:  10.255\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 38.29 GiB (GPU 0; 10.92 GiB total capacity; 579.21 MiB already allocated; 9.35 GiB free; 798.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62082\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.14 GiB (GPU 0; 10.92 GiB total capacity; 566.08 MiB already allocated; 9.32 GiB free; 828.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.57 GiB (GPU 0; 10.92 GiB total capacity; 560.77 MiB already allocated; 9.32 GiB free; 828.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15521\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.79 GiB (GPU 0; 10.92 GiB total capacity; 5.27 GiB already allocated; 4.53 GiB free; 5.60 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7761\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.39 GiB (GPU 0; 10.92 GiB total capacity; 7.76 GiB already allocated; 2.13 GiB free; 7.99 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3881\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 10.92 GiB total capacity; 8.94 GiB already allocated; 961.38 MiB free; 9.19 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1941\n",
      "Iter 1/1 - Loss: 12.539   lengthscale: 40.313   noise: 3.579\n",
      "Finished training on 124164 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.536   lengthscale: 40.313   noise: 3.579\n",
      "Iter 2/100 - Loss: 11.883   lengthscale: 40.304   noise: 3.905\n",
      "Iter 3/100 - Loss: 11.142   lengthscale: 40.286   noise: 4.336\n",
      "Iter 4/100 - Loss: 10.487   lengthscale: 40.259   noise: 4.789\n",
      "Iter 5/100 - Loss: 9.890   lengthscale: 40.223   noise: 5.277\n",
      "Iter 6/100 - Loss: 9.341   lengthscale: 40.174   noise: 5.805\n",
      "Iter 7/100 - Loss: 8.836   lengthscale: 40.108   noise: 6.376\n",
      "Iter 8/100 - Loss: 8.369   lengthscale: 40.024   noise: 6.993\n",
      "Iter 9/100 - Loss: 7.938   lengthscale: 39.917   noise: 7.662\n",
      "Iter 10/100 - Loss: 7.538   lengthscale: 39.783   noise: 8.385\n",
      "Iter 11/100 - Loss: 7.165   lengthscale: 39.616   noise: 9.168\n",
      "Iter 12/100 - Loss: 6.820   lengthscale: 39.398   noise: 10.013\n",
      "Iter 13/100 - Loss: 6.503   lengthscale: 39.115   noise: 10.919\n",
      "Iter 14/100 - Loss: 6.209   lengthscale: 38.749   noise: 11.893\n",
      "Iter 15/100 - Loss: 5.937   lengthscale: 38.279   noise: 12.940\n",
      "Iter 16/100 - Loss: 5.687   lengthscale: 37.681   noise: 14.059\n",
      "Iter 17/100 - Loss: 5.455   lengthscale: 36.921   noise: 15.254\n",
      "Iter 18/100 - Loss: 5.242   lengthscale: 35.956   noise: 16.529\n",
      "Iter 19/100 - Loss: 5.042   lengthscale: 34.718   noise: 17.891\n",
      "Iter 20/100 - Loss: 4.857   lengthscale: 33.145   noise: 19.336\n",
      "Iter 21/100 - Loss: 4.681   lengthscale: 31.092   noise: 20.888\n",
      "Iter 22/100 - Loss: 4.510   lengthscale: 28.367   noise: 22.564\n",
      "Iter 23/100 - Loss: 4.332   lengthscale: 24.538   noise: 24.443\n",
      "Iter 24/100 - Loss: 3.675   lengthscale: 1.703   noise: 33.356\n",
      "Iter 25/100 - Loss: 3.543   lengthscale: 3.643   noise: 32.640\n",
      "Iter 26/100 - Loss: 3.543   lengthscale: 3.643   noise: 32.640\n",
      "Convergence reached!\n",
      "Finished training on 124164 data points using 3 GPUs.\n",
      "3100198 MAE:  16.616 ME 16.445 STD:  7.686\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 38.70 GiB (GPU 0; 10.92 GiB total capacity; 579.57 MiB already allocated; 9.22 GiB free; 926.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.35 GiB (GPU 0; 10.92 GiB total capacity; 568.82 MiB already allocated; 9.25 GiB free; 902.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31206\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.67 GiB (GPU 0; 10.92 GiB total capacity; 563.35 MiB already allocated; 9.25 GiB free; 902.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15603\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.84 GiB (GPU 0; 10.92 GiB total capacity; 5.32 GiB already allocated; 4.41 GiB free; 5.72 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7802\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.42 GiB (GPU 0; 10.92 GiB total capacity; 7.84 GiB already allocated; 1.99 GiB free; 8.14 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3901\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.21 GiB (GPU 1; 10.92 GiB total capacity; 9.78 GiB already allocated; 399.38 MiB free; 9.79 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1951\n",
      "Iter 1/1 - Loss: 12.490   lengthscale: 39.988   noise: 3.518\n",
      "Finished training on 124821 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.491   lengthscale: 39.988   noise: 3.518\n",
      "Iter 2/100 - Loss: 11.818   lengthscale: 39.979   noise: 3.848\n",
      "Iter 3/100 - Loss: 11.079   lengthscale: 39.961   noise: 4.273\n",
      "Iter 5/100 - Loss: 9.829   lengthscale: 39.902   noise: 5.204\n",
      "Iter 6/100 - Loss: 9.284   lengthscale: 39.855   noise: 5.726\n",
      "Iter 7/100 - Loss: 8.781   lengthscale: 39.793   noise: 6.291\n",
      "Iter 8/100 - Loss: 8.316   lengthscale: 39.714   noise: 6.904\n",
      "Iter 9/100 - Loss: 7.890   lengthscale: 39.612   noise: 7.559\n",
      "Iter 10/100 - Loss: 7.493   lengthscale: 39.484   noise: 8.273\n",
      "Iter 11/100 - Loss: 7.125   lengthscale: 39.326   noise: 9.043\n",
      "Iter 12/100 - Loss: 6.785   lengthscale: 39.118   noise: 9.875\n",
      "Iter 13/100 - Loss: 6.470   lengthscale: 38.847   noise: 10.770\n",
      "Iter 14/100 - Loss: 6.177   lengthscale: 38.497   noise: 11.738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15/100 - Loss: 5.910   lengthscale: 38.053   noise: 12.766\n",
      "Iter 16/100 - Loss: 5.661   lengthscale: 37.483   noise: 13.875\n",
      "Iter 17/100 - Loss: 5.433   lengthscale: 36.765   noise: 15.052\n",
      "Iter 18/100 - Loss: 5.221   lengthscale: 35.850   noise: 16.312\n",
      "Iter 19/100 - Loss: 5.025   lengthscale: 34.681   noise: 17.657\n",
      "Iter 20/100 - Loss: 4.844   lengthscale: 33.193   noise: 19.082\n",
      "Iter 21/100 - Loss: 4.671   lengthscale: 31.258   noise: 20.614\n",
      "Iter 22/100 - Loss: 4.504   lengthscale: 28.702   noise: 22.262\n",
      "Iter 24/100 - Loss: 4.139   lengthscale: 19.877   noise: 26.243\n",
      "Iter 25/100 - Loss: 3.560   lengthscale: 3.008   noise: 31.820\n",
      "Iter 26/100 - Loss: 3.549   lengthscale: 2.849   noise: 31.813\n",
      "Iter 27/100 - Loss: 3.516   lengthscale: 2.809   noise: 31.170\n",
      "Iter 28/100 - Loss: 3.518   lengthscale: 2.809   noise: 31.170\n",
      "Convergence reached!\n",
      "Finished training on 124821 data points using 3 GPUs.\n",
      "3100237 MAE:  16.866 ME -14.965 STD:  12.821\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.80 GiB (GPU 0; 10.92 GiB total capacity; 578.45 MiB already allocated; 9.25 GiB free; 904.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 63296\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.90 GiB (GPU 0; 10.92 GiB total capacity; 571.79 MiB already allocated; 9.28 GiB free; 870.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31648\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.95 GiB (GPU 0; 10.92 GiB total capacity; 566.40 MiB already allocated; 9.36 GiB free; 790.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15824\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.98 GiB (GPU 0; 10.92 GiB total capacity; 5.46 GiB already allocated; 4.38 GiB free; 5.75 GiB reserved in total by PyTorch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 3 -- Kernel partition size: 7912\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.49 GiB (GPU 0; 10.92 GiB total capacity; 8.05 GiB already allocated; 1.82 GiB free; 8.31 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3956\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 10.92 GiB total capacity; 9.27 GiB already allocated; 585.38 MiB free; 9.56 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1978\n",
      "Iter 1/1 - Loss: 12.093   lengthscale: 39.820   noise: 3.439\n",
      "Finished training on 126591 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.092   lengthscale: 39.820   noise: 3.439\n",
      "Iter 2/100 - Loss: 11.455   lengthscale: 39.811   noise: 3.758\n",
      "Iter 3/100 - Loss: 10.748   lengthscale: 39.793   noise: 4.176\n",
      "Iter 4/100 - Loss: 10.122   lengthscale: 39.767   noise: 4.617\n",
      "Iter 5/100 - Loss: 9.546   lengthscale: 39.731   noise: 5.091\n",
      "Iter 6/100 - Loss: 9.026   lengthscale: 39.683   noise: 5.602\n",
      "Iter 7/100 - Loss: 8.546   lengthscale: 39.618   noise: 6.154\n",
      "Iter 8/100 - Loss: 8.102   lengthscale: 39.535   noise: 6.754\n",
      "Iter 9/100 - Loss: 7.688   lengthscale: 39.427   noise: 7.400\n",
      "Iter 10/100 - Loss: 7.307   lengthscale: 39.294   noise: 8.100\n",
      "Iter 11/100 - Loss: 6.953   lengthscale: 39.129   noise: 8.856\n",
      "Iter 12/100 - Loss: 6.624   lengthscale: 38.912   noise: 9.671\n",
      "Iter 13/100 - Loss: 6.319   lengthscale: 38.628   noise: 10.549\n",
      "Iter 14/100 - Loss: 6.039   lengthscale: 38.264   noise: 11.487\n",
      "Iter 15/100 - Loss: 5.779   lengthscale: 37.795   noise: 12.498\n",
      "Iter 16/100 - Loss: 5.539   lengthscale: 37.199   noise: 13.574\n",
      "Iter 17/100 - Loss: 5.318   lengthscale: 36.440   noise: 14.726\n",
      "Iter 18/100 - Loss: 5.114   lengthscale: 35.477   noise: 15.947\n",
      "Iter 19/100 - Loss: 4.925   lengthscale: 34.249   noise: 17.250\n",
      "Iter 20/100 - Loss: 4.747   lengthscale: 32.673   noise: 18.638\n",
      "Iter 21/100 - Loss: 4.577   lengthscale: 30.612   noise: 20.128\n",
      "Iter 22/100 - Loss: 4.413   lengthscale: 27.882   noise: 21.730\n",
      "Iter 23/100 - Loss: 4.241   lengthscale: 24.064   noise: 23.512\n",
      "Iter 24/100 - Loss: 3.480   lengthscale: 3.225   noise: 31.171\n",
      "Iter 25/100 - Loss: 3.489   lengthscale: 3.225   noise: 31.171\n",
      "Convergence reached!\n",
      "Finished training on 126591 data points using 3 GPUs.\n",
      "3100240 MAE:  29.272 ME -29.083 STD:  12.144\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.48 GiB (GPU 0; 10.92 GiB total capacity; 579.86 MiB already allocated; 9.28 GiB free; 870.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 63043\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.74 GiB (GPU 0; 10.92 GiB total capacity; 571.82 MiB already allocated; 9.29 GiB free; 860.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31522\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.87 GiB (GPU 0; 10.92 GiB total capacity; 566.29 MiB already allocated; 9.29 GiB free; 860.00 MiB reserved in total by PyTorch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 3 -- Kernel partition size: 15761\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.94 GiB (GPU 0; 10.92 GiB total capacity; 5.42 GiB already allocated; 4.42 GiB free; 5.71 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7881\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.47 GiB (GPU 0; 10.92 GiB total capacity; 7.99 GiB already allocated; 1.88 GiB free; 8.25 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3941\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 10.92 GiB total capacity; 9.21 GiB already allocated; 649.38 MiB free; 9.49 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1971\n",
      "Iter 1/1 - Loss: 12.565   lengthscale: 39.904   noise: 3.506\n",
      "Finished training on 126086 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.564   lengthscale: 39.904   noise: 3.506\n",
      "Iter 2/100 - Loss: 11.884   lengthscale: 39.895   noise: 3.837\n",
      "Iter 3/100 - Loss: 11.143   lengthscale: 39.877   noise: 4.261\n",
      "Iter 4/100 - Loss: 10.489   lengthscale: 39.851   noise: 4.707\n",
      "Iter 5/100 - Loss: 9.888   lengthscale: 39.815   noise: 5.190\n",
      "Iter 6/100 - Loss: 9.340   lengthscale: 39.766   noise: 5.711\n",
      "Iter 7/100 - Loss: 8.834   lengthscale: 39.702   noise: 6.274\n",
      "Iter 8/100 - Loss: 8.367   lengthscale: 39.618   noise: 6.884\n",
      "Iter 9/100 - Loss: 7.936   lengthscale: 39.512   noise: 7.543\n",
      "Iter 10/100 - Loss: 7.533   lengthscale: 39.379   noise: 8.261\n",
      "Iter 11/100 - Loss: 7.162   lengthscale: 39.217   noise: 9.032\n",
      "Iter 12/100 - Loss: 6.818   lengthscale: 39.003   noise: 9.863\n",
      "Iter 13/100 - Loss: 6.498   lengthscale: 38.722   noise: 10.762\n",
      "Iter 14/100 - Loss: 6.204   lengthscale: 38.362   noise: 11.725\n",
      "Iter 15/100 - Loss: 5.933   lengthscale: 37.901   noise: 12.754\n",
      "Iter 16/100 - Loss: 5.681   lengthscale: 37.312   noise: 13.859\n",
      "Iter 17/100 - Loss: 5.449   lengthscale: 36.561   noise: 15.038\n",
      "Iter 18/100 - Loss: 5.236   lengthscale: 35.611   noise: 16.287\n",
      "Iter 19/100 - Loss: 5.037   lengthscale: 34.388   noise: 17.633\n",
      "Iter 20/100 - Loss: 4.851   lengthscale: 32.829   noise: 19.057\n",
      "Iter 21/100 - Loss: 4.675   lengthscale: 30.801   noise: 20.583\n",
      "Iter 22/100 - Loss: 4.502   lengthscale: 28.084   noise: 22.244\n",
      "Iter 23/100 - Loss: 4.324   lengthscale: 24.314   noise: 24.080\n",
      "Iter 24/100 - Loss: 4.108   lengthscale: 18.416   noise: 26.342\n",
      "Iter 25/100 - Loss: 3.614   lengthscale: 1.927   noise: 31.625\n",
      "Iter 26/100 - Loss: 3.540   lengthscale: 3.296   noise: 31.212\n",
      "Iter 27/100 - Loss: 3.558   lengthscale: 3.296   noise: 31.212\n",
      "Convergence reached!\n",
      "Finished training on 126086 data points using 3 GPUs.\n",
      "3100308 MAE:  20.840 ME 19.669 STD:  10.529\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.39 GiB (GPU 0; 10.92 GiB total capacity; 582.34 MiB already allocated; 9.29 GiB free; 860.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62966\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.69 GiB (GPU 0; 10.92 GiB total capacity; 570.41 MiB already allocated; 9.28 GiB free; 872.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.85 GiB (GPU 0; 10.92 GiB total capacity; 565.12 MiB already allocated; 9.28 GiB free; 872.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15742\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.93 GiB (GPU 0; 10.92 GiB total capacity; 5.41 GiB already allocated; 4.43 GiB free; 5.70 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7871\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.46 GiB (GPU 0; 10.92 GiB total capacity; 7.97 GiB already allocated; 1.89 GiB free; 8.24 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3936\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 2; 10.92 GiB total capacity; 9.95 GiB already allocated; 235.38 MiB free; 9.97 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1968\n",
      "Iter 1/1 - Loss: 12.598   lengthscale: 38.517   noise: 3.456\n",
      "Finished training on 125932 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.598   lengthscale: 38.517   noise: 3.456\n",
      "Iter 2/100 - Loss: 11.905   lengthscale: 38.507   noise: 3.790\n",
      "Iter 3/100 - Loss: 11.157   lengthscale: 38.488   noise: 4.215\n",
      "Iter 4/100 - Loss: 10.496   lengthscale: 38.462   noise: 4.661\n",
      "Iter 5/100 - Loss: 9.893   lengthscale: 38.426   noise: 5.143\n",
      "Iter 6/100 - Loss: 9.340   lengthscale: 38.377   noise: 5.662\n",
      "Iter 7/100 - Loss: 8.833   lengthscale: 38.313   noise: 6.223\n",
      "Iter 8/100 - Loss: 8.364   lengthscale: 38.230   noise: 6.829\n",
      "Iter 9/100 - Loss: 7.931   lengthscale: 38.124   noise: 7.484\n",
      "Iter 10/100 - Loss: 7.529   lengthscale: 37.991   noise: 8.193\n",
      "Iter 11/100 - Loss: 7.157   lengthscale: 37.829   noise: 8.958\n",
      "Iter 12/100 - Loss: 6.812   lengthscale: 37.613   noise: 9.783\n",
      "Iter 13/100 - Loss: 6.492   lengthscale: 37.330   noise: 10.672\n",
      "Iter 14/100 - Loss: 6.198   lengthscale: 36.966   noise: 11.624\n",
      "Iter 15/100 - Loss: 5.925   lengthscale: 36.497   noise: 12.649\n",
      "Iter 16/100 - Loss: 5.674   lengthscale: 35.901   noise: 13.739\n",
      "Iter 17/100 - Loss: 5.443   lengthscale: 35.139   noise: 14.907\n",
      "Iter 18/100 - Loss: 5.228   lengthscale: 34.172   noise: 16.154\n",
      "Iter 19/100 - Loss: 5.028   lengthscale: 32.935   noise: 17.487\n",
      "Iter 20/100 - Loss: 4.842   lengthscale: 31.350   noise: 18.901\n",
      "Iter 21/100 - Loss: 4.663   lengthscale: 29.268   noise: 20.428\n",
      "Iter 22/100 - Loss: 4.487   lengthscale: 26.481   noise: 22.081\n",
      "Iter 23/100 - Loss: 4.302   lengthscale: 22.552   noise: 23.931\n",
      "Iter 24/100 - Loss: 3.530   lengthscale: 3.187   noise: 31.126\n",
      "Iter 25/100 - Loss: 3.526   lengthscale: 2.706   noise: 31.249\n",
      "Iter 26/100 - Loss: 3.522   lengthscale: 2.736   noise: 31.239\n",
      "Iter 27/100 - Loss: 3.523   lengthscale: 2.736   noise: 31.239\n",
      "Convergence reached!\n",
      "Finished training on 125932 data points using 3 GPUs.\n",
      "3100331 MAE:  17.871 ME 16.900 STD:  14.103\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.65 GiB (GPU 0; 10.92 GiB total capacity; 581.74 MiB already allocated; 9.27 GiB free; 874.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 63174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.82 GiB (GPU 0; 10.92 GiB total capacity; 573.05 MiB already allocated; 9.36 GiB free; 790.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31587\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.91 GiB (GPU 0; 10.92 GiB total capacity; 567.67 MiB already allocated; 9.36 GiB free; 790.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15794\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.96 GiB (GPU 0; 10.92 GiB total capacity; 5.44 GiB already allocated; 4.40 GiB free; 5.73 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7897\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.48 GiB (GPU 0; 10.92 GiB total capacity; 8.02 GiB already allocated; 1.83 GiB free; 8.30 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3949\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.24 GiB (GPU 1; 10.92 GiB total capacity; 10.02 GiB already allocated; 147.38 MiB free; 10.04 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1975\n",
      "Iter 1/1 - Loss: 12.614   lengthscale: 39.499   noise: 3.531\n",
      "Finished training on 126347 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.617   lengthscale: 39.499   noise: 3.531\n",
      "Iter 2/100 - Loss: 11.940   lengthscale: 39.489   noise: 3.861\n",
      "Iter 3/100 - Loss: 11.193   lengthscale: 39.471   noise: 4.291\n",
      "Iter 4/100 - Loss: 10.533   lengthscale: 39.445   noise: 4.742\n",
      "Iter 5/100 - Loss: 9.931   lengthscale: 39.408   noise: 5.229\n",
      "Iter 6/100 - Loss: 9.377   lengthscale: 39.359   noise: 5.755\n",
      "Iter 7/100 - Loss: 8.869   lengthscale: 39.294   noise: 6.322\n",
      "Iter 8/100 - Loss: 8.399   lengthscale: 39.209   noise: 6.937\n",
      "Iter 9/100 - Loss: 7.963   lengthscale: 39.102   noise: 7.602\n",
      "Iter 10/100 - Loss: 7.561   lengthscale: 38.968   noise: 8.320\n",
      "Iter 11/100 - Loss: 7.187   lengthscale: 38.803   noise: 9.097\n",
      "Iter 12/100 - Loss: 6.841   lengthscale: 38.584   noise: 9.936\n",
      "Iter 13/100 - Loss: 6.520   lengthscale: 38.299   noise: 10.837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14/100 - Loss: 6.224   lengthscale: 37.930   noise: 11.806\n",
      "Iter 15/100 - Loss: 5.950   lengthscale: 37.458   noise: 12.846\n",
      "Iter 16/100 - Loss: 5.696   lengthscale: 36.853   noise: 13.960\n",
      "Iter 17/100 - Loss: 5.464   lengthscale: 36.089   noise: 15.142\n",
      "Iter 18/100 - Loss: 5.249   lengthscale: 35.113   noise: 16.408\n",
      "Iter 19/100 - Loss: 5.048   lengthscale: 33.864   noise: 17.759\n",
      "Iter 20/100 - Loss: 4.860   lengthscale: 32.260   noise: 19.199\n",
      "Iter 21/100 - Loss: 4.682   lengthscale: 30.163   noise: 20.747\n",
      "Iter 22/100 - Loss: 4.506   lengthscale: 27.369   noise: 22.417\n",
      "Iter 23/100 - Loss: 4.322   lengthscale: 23.409   noise: 24.300\n",
      "Iter 24/100 - Loss: 3.543   lengthscale: 3.466   noise: 31.778\n",
      "Iter 25/100 - Loss: 3.542   lengthscale: 3.466   noise: 31.778\n",
      "Convergence reached!\n",
      "Finished training on 126347 data points using 3 GPUs.\n",
      "3100524 MAE:  10.125 ME -3.294 STD:  12.166\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 38.29 GiB (GPU 0; 10.92 GiB total capacity; 580.34 MiB already allocated; 9.27 GiB free; 880.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62079\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.14 GiB (GPU 0; 10.92 GiB total capacity; 566.56 MiB already allocated; 9.36 GiB free; 788.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31040\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.57 GiB (GPU 0; 10.92 GiB total capacity; 560.98 MiB already allocated; 9.36 GiB free; 788.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15520\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.79 GiB (GPU 0; 10.92 GiB total capacity; 5.27 GiB already allocated; 4.57 GiB free; 5.56 GiB reserved in total by PyTorch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 3 -- Kernel partition size: 7760\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.39 GiB (GPU 0; 10.92 GiB total capacity; 7.76 GiB already allocated; 2.10 GiB free; 8.03 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3880\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 1; 10.92 GiB total capacity; 9.68 GiB already allocated; 507.38 MiB free; 9.68 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1940\n",
      "Iter 1/1 - Loss: 12.856   lengthscale: 39.735   noise: 3.483\n",
      "Finished training on 124157 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.856   lengthscale: 39.735   noise: 3.483\n",
      "Iter 2/100 - Loss: 12.125   lengthscale: 39.725   noise: 3.827\n",
      "Iter 3/100 - Loss: 11.367   lengthscale: 39.708   noise: 4.248\n",
      "Iter 4/100 - Loss: 10.691   lengthscale: 39.683   noise: 4.695\n",
      "Iter 5/100 - Loss: 10.074   lengthscale: 39.649   noise: 5.176\n",
      "Iter 6/100 - Loss: 9.511   lengthscale: 39.601   noise: 5.695\n",
      "Iter 7/100 - Loss: 8.991   lengthscale: 39.539   noise: 6.257\n",
      "Iter 8/100 - Loss: 8.512   lengthscale: 39.458   noise: 6.864\n",
      "Iter 9/100 - Loss: 8.070   lengthscale: 39.356   noise: 7.520\n",
      "Iter 10/100 - Loss: 7.657   lengthscale: 39.227   noise: 8.232\n",
      "Iter 11/100 - Loss: 7.276   lengthscale: 39.068   noise: 9.002\n",
      "Iter 12/100 - Loss: 6.924   lengthscale: 38.858   noise: 9.832\n",
      "Iter 13/100 - Loss: 6.597   lengthscale: 38.586   noise: 10.728\n",
      "Iter 14/100 - Loss: 6.294   lengthscale: 38.234   noise: 11.689\n",
      "Iter 15/100 - Loss: 6.016   lengthscale: 37.784   noise: 12.717\n",
      "Iter 16/100 - Loss: 5.758   lengthscale: 37.209   noise: 13.822\n",
      "Iter 17/100 - Loss: 5.521   lengthscale: 36.482   noise: 14.997\n",
      "Iter 18/100 - Loss: 5.302   lengthscale: 35.554   noise: 16.256\n",
      "Iter 19/100 - Loss: 5.099   lengthscale: 34.375   noise: 17.598\n",
      "Iter 20/100 - Loss: 4.909   lengthscale: 32.866   noise: 19.028\n",
      "Iter 21/100 - Loss: 4.729   lengthscale: 30.907   noise: 20.561\n",
      "Iter 22/100 - Loss: 4.555   lengthscale: 28.311   noise: 22.217\n",
      "Iter 23/100 - Loss: 4.377   lengthscale: 24.748   noise: 24.036\n",
      "Iter 24/100 - Loss: 4.168   lengthscale: 19.249   noise: 26.252\n",
      "Iter 25/100 - Loss: 3.577   lengthscale: 2.483   noise: 31.774\n",
      "Iter 26/100 - Loss: 3.561   lengthscale: 3.635   noise: 31.465\n",
      "Iter 27/100 - Loss: 3.570   lengthscale: 3.635   noise: 31.465\n",
      "Convergence reached!\n",
      "Finished training on 124157 data points using 3 GPUs.\n",
      "3100757 MAE:  28.412 ME 28.371 STD:  11.355\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.39 GiB (GPU 0; 10.92 GiB total capacity; 581.91 MiB already allocated; 9.28 GiB free; 870.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62966\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.69 GiB (GPU 0; 10.92 GiB total capacity; 570.27 MiB already allocated; 9.36 GiB free; 788.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.85 GiB (GPU 0; 10.92 GiB total capacity; 564.91 MiB already allocated; 9.36 GiB free; 788.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15742\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.93 GiB (GPU 0; 10.92 GiB total capacity; 5.41 GiB already allocated; 4.43 GiB free; 5.69 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7871\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.46 GiB (GPU 0; 10.92 GiB total capacity; 7.97 GiB already allocated; 1.88 GiB free; 8.24 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3936\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 1; 10.92 GiB total capacity; 9.95 GiB already allocated; 215.38 MiB free; 9.97 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1968\n",
      "Iter 1/1 - Loss: 12.726   lengthscale: 39.978   noise: 3.543\n",
      "Finished training on 125931 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.724   lengthscale: 39.977   noise: 3.543\n",
      "Iter 2/100 - Loss: 12.033   lengthscale: 39.968   noise: 3.878\n",
      "Iter 3/100 - Loss: 11.281   lengthscale: 39.950   noise: 4.307\n",
      "Iter 4/100 - Loss: 10.613   lengthscale: 39.924   noise: 4.757\n",
      "Iter 5/100 - Loss: 10.004   lengthscale: 39.889   noise: 5.245\n",
      "Iter 6/100 - Loss: 9.445   lengthscale: 39.840   noise: 5.771\n",
      "Iter 7/100 - Loss: 8.932   lengthscale: 39.777   noise: 6.339\n",
      "Iter 8/100 - Loss: 8.458   lengthscale: 39.694   noise: 6.954\n",
      "Iter 9/100 - Loss: 8.019   lengthscale: 39.589   noise: 7.618\n",
      "Iter 10/100 - Loss: 7.612   lengthscale: 39.457   noise: 8.339\n",
      "Iter 11/100 - Loss: 7.235   lengthscale: 39.295   noise: 9.118\n",
      "Iter 12/100 - Loss: 6.886   lengthscale: 39.080   noise: 9.957\n",
      "Iter 13/100 - Loss: 6.562   lengthscale: 38.799   noise: 10.863\n",
      "Iter 14/100 - Loss: 6.262   lengthscale: 38.438   noise: 11.836\n",
      "Iter 15/100 - Loss: 5.986   lengthscale: 37.975   noise: 12.879\n",
      "Iter 16/100 - Loss: 5.733   lengthscale: 37.389   noise: 13.989\n",
      "Iter 17/100 - Loss: 5.496   lengthscale: 36.635   noise: 15.187\n",
      "Iter 18/100 - Loss: 5.280   lengthscale: 35.685   noise: 16.457\n",
      "Iter 19/100 - Loss: 5.078   lengthscale: 34.473   noise: 17.811\n",
      "Iter 20/100 - Loss: 4.889   lengthscale: 32.913   noise: 19.259\n",
      "Iter 21/100 - Loss: 4.711   lengthscale: 30.892   noise: 20.805\n",
      "Iter 22/100 - Loss: 4.537   lengthscale: 28.202   noise: 22.479\n",
      "Iter 23/100 - Loss: 4.357   lengthscale: 24.448   noise: 24.343\n",
      "Iter 24/100 - Loss: 4.141   lengthscale: 18.641   noise: 26.620\n",
      "Iter 25/100 - Loss: 3.541   lengthscale: 2.454   noise: 31.891\n",
      "Iter 26/100 - Loss: 3.532   lengthscale: 2.454   noise: 31.891\n",
      "Convergence reached!\n",
      "Finished training on 125931 data points using 3 GPUs.\n",
      "3100835 MAE:  10.521 ME -1.903 STD:  12.520\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 38.85 GiB (GPU 0; 10.92 GiB total capacity; 578.69 MiB already allocated; 9.27 GiB free; 876.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.43 GiB (GPU 0; 10.92 GiB total capacity; 569.52 MiB already allocated; 9.18 GiB free; 966.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31268\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.71 GiB (GPU 0; 10.92 GiB total capacity; 563.71 MiB already allocated; 9.18 GiB free; 966.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15634\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.86 GiB (GPU 0; 10.92 GiB total capacity; 5.34 GiB already allocated; 4.41 GiB free; 5.72 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7817\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.43 GiB (GPU 0; 10.92 GiB total capacity; 7.87 GiB already allocated; 1.89 GiB free; 8.23 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3909\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.21 GiB (GPU 2; 10.92 GiB total capacity; 8.60 GiB already allocated; 403.38 MiB free; 9.81 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1955\n",
      "Iter 1/1 - Loss: 12.719   lengthscale: 39.983   noise: 3.485\n",
      "Finished training on 125069 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.717   lengthscale: 39.983   noise: 3.485\n",
      "Iter 2/100 - Loss: 12.012   lengthscale: 39.974   noise: 3.822\n",
      "Iter 3/100 - Loss: 11.262   lengthscale: 39.956   noise: 4.246\n",
      "Iter 4/100 - Loss: 10.595   lengthscale: 39.931   noise: 4.692\n",
      "Iter 5/100 - Loss: 9.988   lengthscale: 39.895   noise: 5.173\n",
      "Iter 6/100 - Loss: 9.431   lengthscale: 39.847   noise: 5.693\n",
      "Iter 7/100 - Loss: 8.919   lengthscale: 39.784   noise: 6.254\n",
      "Iter 8/100 - Loss: 8.446   lengthscale: 39.701   noise: 6.861\n",
      "Iter 9/100 - Loss: 8.009   lengthscale: 39.596   noise: 7.519\n",
      "Iter 10/100 - Loss: 7.602   lengthscale: 39.465   noise: 8.231\n",
      "Iter 11/100 - Loss: 7.224   lengthscale: 39.302   noise: 9.001\n",
      "Iter 12/100 - Loss: 6.876   lengthscale: 39.089   noise: 9.829\n",
      "Iter 13/100 - Loss: 6.554   lengthscale: 38.810   noise: 10.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14/100 - Loss: 6.254   lengthscale: 38.448   noise: 11.685\n",
      "Iter 15/100 - Loss: 5.978   lengthscale: 37.986   noise: 12.713\n",
      "Iter 16/100 - Loss: 5.724   lengthscale: 37.398   noise: 13.813\n",
      "Iter 17/100 - Loss: 5.487   lengthscale: 36.649   noise: 14.991\n",
      "Iter 18/100 - Loss: 5.272   lengthscale: 35.706   noise: 16.241\n",
      "Iter 19/100 - Loss: 5.069   lengthscale: 34.489   noise: 17.587\n",
      "Iter 20/100 - Loss: 4.881   lengthscale: 32.939   noise: 19.011\n",
      "Iter 21/100 - Loss: 4.701   lengthscale: 30.919   noise: 20.541\n",
      "Iter 22/100 - Loss: 4.527   lengthscale: 28.240   noise: 22.191\n",
      "Iter 23/100 - Loss: 4.348   lengthscale: 24.523   noise: 24.017\n",
      "Iter 24/100 - Loss: 4.132   lengthscale: 18.730   noise: 26.259\n",
      "Iter 25/100 - Loss: 3.568   lengthscale: 3.308   noise: 31.174\n",
      "Iter 26/100 - Loss: 3.542   lengthscale: 3.138   noise: 31.219\n",
      "Iter 27/100 - Loss: 3.541   lengthscale: 3.107   noise: 31.224\n",
      "Iter 28/100 - Loss: 3.536   lengthscale: 3.057   noise: 31.233\n",
      "Iter 29/100 - Loss: 3.556   lengthscale: 3.057   noise: 31.233\n",
      "Convergence reached!\n",
      "Finished training on 125069 data points using 3 GPUs.\n",
      "3101159 MAE:  12.324 ME 3.779 STD:  14.703\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 38.96 GiB (GPU 0; 10.92 GiB total capacity; 581.28 MiB already allocated; 9.18 GiB free; 968.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 62623\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.48 GiB (GPU 0; 10.92 GiB total capacity; 569.94 MiB already allocated; 9.27 GiB free; 882.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31312\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.74 GiB (GPU 0; 10.92 GiB total capacity; 564.27 MiB already allocated; 9.27 GiB free; 882.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15656\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.87 GiB (GPU 0; 10.92 GiB total capacity; 5.35 GiB already allocated; 4.40 GiB free; 5.73 GiB reserved in total by PyTorch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 3 -- Kernel partition size: 7828\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 10.92 GiB total capacity; 7.89 GiB already allocated; 1.96 GiB free; 8.17 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3914\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 1; 10.92 GiB total capacity; 8.63 GiB already allocated; 347.38 MiB free; 9.84 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1957\n",
      "Iter 1/1 - Loss: 12.837   lengthscale: 40.071   noise: 3.548\n",
      "Finished training on 125246 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.836   lengthscale: 40.071   noise: 3.548\n",
      "Iter 2/100 - Loss: 12.130   lengthscale: 40.061   noise: 3.886\n",
      "Iter 3/100 - Loss: 11.372   lengthscale: 40.043   noise: 4.314\n",
      "Iter 4/100 - Loss: 10.700   lengthscale: 40.018   noise: 4.766\n",
      "Iter 5/100 - Loss: 10.083   lengthscale: 39.982   noise: 5.253\n",
      "Iter 6/100 - Loss: 9.521   lengthscale: 39.934   noise: 5.780\n",
      "Iter 7/100 - Loss: 9.001   lengthscale: 39.870   noise: 6.349\n",
      "Iter 8/100 - Loss: 8.522   lengthscale: 39.787   noise: 6.966\n",
      "Iter 9/100 - Loss: 8.079   lengthscale: 39.682   noise: 7.632\n",
      "Iter 10/100 - Loss: 7.666   lengthscale: 39.549   noise: 8.354\n",
      "Iter 11/100 - Loss: 7.285   lengthscale: 39.387   noise: 9.134\n",
      "Iter 12/100 - Loss: 6.931   lengthscale: 39.172   noise: 9.976\n",
      "Iter 13/100 - Loss: 6.604   lengthscale: 38.892   noise: 10.882\n",
      "Iter 14/100 - Loss: 6.302   lengthscale: 38.531   noise: 11.856\n",
      "Iter 15/100 - Loss: 6.023   lengthscale: 38.068   noise: 12.902\n",
      "Iter 16/100 - Loss: 5.766   lengthscale: 37.481   noise: 14.018\n",
      "Iter 17/100 - Loss: 5.526   lengthscale: 36.727   noise: 15.218\n",
      "Iter 18/100 - Loss: 5.308   lengthscale: 35.781   noise: 16.485\n",
      "Iter 19/100 - Loss: 5.104   lengthscale: 34.567   noise: 17.847\n",
      "Iter 20/100 - Loss: 4.913   lengthscale: 33.008   noise: 19.301\n",
      "Iter 21/100 - Loss: 4.732   lengthscale: 30.984   noise: 20.858\n",
      "Iter 22/100 - Loss: 4.556   lengthscale: 28.299   noise: 22.539\n",
      "Iter 23/100 - Loss: 4.375   lengthscale: 24.559   noise: 24.410\n",
      "Iter 24/100 - Loss: 4.159   lengthscale: 18.798   noise: 26.679\n",
      "Iter 25/100 - Loss: 3.573   lengthscale: 3.043   noise: 31.807\n",
      "Iter 26/100 - Loss: 3.552   lengthscale: 2.785   noise: 31.846\n",
      "Iter 27/100 - Loss: 3.551   lengthscale: 2.785   noise: 31.846\n",
      "Convergence reached!\n",
      "Finished training on 125246 data points using 3 GPUs.\n",
      "3101412 MAE:  7.947 ME 3.808 STD:  9.252\n",
      "Number of devices: 3 -- Kernel partition size: 0\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 39.45 GiB (GPU 0; 10.92 GiB total capacity; 580.39 MiB already allocated; 9.27 GiB free; 884.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 63019\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 19.73 GiB (GPU 0; 10.92 GiB total capacity; 570.54 MiB already allocated; 9.18 GiB free; 970.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 31510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 111, in forward\n",
      "    return MaternCovariance().apply(\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/functions/matern_covariance.py\", line 19, in forward\n",
      "    scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 112, in <lambda>\n",
      "    x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 329, in covar_dist\n",
      "    res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 54, in _dist\n",
      "    res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 43, in _sq_dist\n",
      "    res = x1_.matmul(x2_.transpose(-2, -1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 9.87 GiB (GPU 0; 10.92 GiB total capacity; 565.96 MiB already allocated; 9.18 GiB free; 970.00 MiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 15755\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 101, in forward\n",
      "    return orig_output.mul(outputscales)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 4.93 GiB (GPU 0; 10.92 GiB total capacity; 5.42 GiB already allocated; 4.33 GiB free; 5.80 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 7878\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/kernel.py\", line 398, in __call__\n",
      "    res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/module.py\", line 28, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/scale_kernel.py\", line 92, in forward\n",
      "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
      "  File \"/home/staff_qing_liu/.conda/envs/jiacheng/lib/python3.8/site-packages/gpytorch/kernels/matern_kernel.py\", line 102, in forward\n",
      "    exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.47 GiB (GPU 0; 10.92 GiB total capacity; 7.99 GiB already allocated; 1.70 GiB free; 8.43 GiB reserved in total by PyTorch)\n",
      "\n",
      "Number of devices: 3 -- Kernel partition size: 3939\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 1; 10.92 GiB total capacity; 9.97 GiB already allocated; 197.38 MiB free; 9.99 GiB reserved in total by PyTorch)\n",
      "Number of devices: 3 -- Kernel partition size: 1970\n",
      "Iter 1/1 - Loss: 12.512   lengthscale: 40.173   noise: 3.544\n",
      "Finished training on 126038 data points using 3 GPUs.\n",
      "Iter 1/100 - Loss: 12.513   lengthscale: 40.173   noise: 3.544\n",
      "Iter 2/100 - Loss: 11.853   lengthscale: 40.164   noise: 3.870\n",
      "Iter 3/100 - Loss: 11.114   lengthscale: 40.146   noise: 4.299\n",
      "Iter 4/100 - Loss: 10.461   lengthscale: 40.120   noise: 4.750\n",
      "Iter 5/100 - Loss: 9.865   lengthscale: 40.083   noise: 5.236\n",
      "Iter 6/100 - Loss: 9.318   lengthscale: 40.034   noise: 5.761\n",
      "Iter 7/100 - Loss: 8.815   lengthscale: 39.969   noise: 6.329\n",
      "Iter 8/100 - Loss: 8.351   lengthscale: 39.885   noise: 6.942\n",
      "Iter 9/100 - Loss: 7.919   lengthscale: 39.777   noise: 7.607\n",
      "Iter 10/100 - Loss: 7.521   lengthscale: 39.643   noise: 8.325\n",
      "Iter 11/100 - Loss: 7.150   lengthscale: 39.478   noise: 9.103\n",
      "Iter 12/100 - Loss: 6.807   lengthscale: 39.260   noise: 9.941\n",
      "Iter 13/100 - Loss: 6.489   lengthscale: 38.975   noise: 10.844\n",
      "Iter 14/100 - Loss: 6.196   lengthscale: 38.609   noise: 11.812\n",
      "Iter 15/100 - Loss: 5.924   lengthscale: 38.137   noise: 12.853\n",
      "Iter 16/100 - Loss: 5.674   lengthscale: 37.539   noise: 13.962\n",
      "Iter 17/100 - Loss: 5.443   lengthscale: 36.775   noise: 15.152\n",
      "Iter 18/100 - Loss: 5.229   lengthscale: 35.805   noise: 16.420\n",
      "Iter 19/100 - Loss: 5.031   lengthscale: 34.573   noise: 17.765\n",
      "Iter 20/100 - Loss: 4.847   lengthscale: 32.993   noise: 19.199\n",
      "Iter 21/100 - Loss: 4.671   lengthscale: 30.933   noise: 20.738\n",
      "Iter 22/100 - Loss: 4.499   lengthscale: 28.179   noise: 22.408\n",
      "Iter 23/100 - Loss: 4.320   lengthscale: 24.354   noise: 24.257\n",
      "Iter 24/100 - Loss: 3.537   lengthscale: 3.375   noise: 32.242\n",
      "Iter 25/100 - Loss: 3.526   lengthscale: 3.054   noise: 32.343\n",
      "Iter 26/100 - Loss: 3.526   lengthscale: 3.029   noise: 32.352\n"
     ]
    }
   ],
   "source": [
    "MAE=0\n",
    "ME=0\n",
    "STD=0\n",
    "L=len(patient_list)\n",
    "error_vector=[]\n",
    "for id in range(L):\n",
    "    source_data=data[data['patient_id']!=patient_list[id]]\n",
    "    target_data=data[data['patient_id']==patient_list[id]]\n",
    "    \n",
    "    train_x=source_data[feature]\n",
    "    test_x=target_data[feature]\n",
    "    scaler=StandardScaler()\n",
    "    train_x=scaler.fit_transform(train_x)\n",
    "    test_x=scaler.transform(test_x)\n",
    "    \n",
    "    train_x=torch.from_numpy( np.array(train_x) )\n",
    "    train_y=torch.from_numpy( np.array(source_data['SBP']) )\n",
    "    test_x=torch.from_numpy( np.array(test_x) )\n",
    "    test_y=torch.from_numpy( np.array(target_data['SBP']))\n",
    "    \n",
    "    output_device = torch.device('cuda:0')\n",
    "    train_x, train_y = train_x.to(output_device), train_y.to(output_device)\n",
    "    test_x, test_y = test_x.to(output_device), test_y.to(output_device)\n",
    "    \n",
    "    class ExactGPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, n_devices):\n",
    "            super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            base_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "\n",
    "            self.covar_module = gpytorch.kernels.MultiDeviceKernel(\n",
    "                base_covar_module, device_ids=range(n_devices),\n",
    "                output_device=output_device\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def train(train_x,\n",
    "              train_y,\n",
    "              n_devices,\n",
    "              output_device,\n",
    "              checkpoint_size,\n",
    "              preconditioner_size,\n",
    "              n_training_iter,\n",
    "    ):\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(output_device)\n",
    "        model = ExactGPModel(train_x, train_y, likelihood, n_devices).to(output_device)\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = FullBatchLBFGS(model.parameters(), lr=0.1)\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "        with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "             gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(train_x)\n",
    "                loss = -mll(output, train_y)\n",
    "                return loss\n",
    "\n",
    "            loss = closure()\n",
    "            loss.backward()\n",
    "\n",
    "            for i in range(n_training_iter):\n",
    "                options = {'closure': closure, 'current_loss': loss, 'max_ls': 10}\n",
    "                loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "\n",
    "                print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                    i + 1, n_training_iter, loss.item(),\n",
    "                    model.covar_module.module.base_kernel.lengthscale.item(),\n",
    "                    model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "                if fail:\n",
    "                    print('Convergence reached!')\n",
    "                    break\n",
    "\n",
    "        print(f\"Finished training on {train_x.size(0)} data points using {n_devices} GPUs.\")\n",
    "        return model, likelihood\n",
    "    \n",
    "    def find_best_gpu_setting(train_x,\n",
    "                              train_y,\n",
    "                              n_devices,\n",
    "                              output_device,\n",
    "                              preconditioner_size\n",
    "    ):\n",
    "        N = train_x.size(0)\n",
    "\n",
    "        # Find the optimum partition/checkpoint size by decreasing in powers of 2\n",
    "        # Start with no partitioning (size = 0)\n",
    "        settings = [0] + [int(n) for n in np.ceil(N / 2**np.arange(1, np.floor(np.log2(N))))]\n",
    "\n",
    "        for checkpoint_size in settings:\n",
    "            print('Number of devices: {} -- Kernel partition size: {}'.format(n_devices, checkpoint_size))\n",
    "            try:\n",
    "                # Try a full forward and backward pass with this setting to check memory usage\n",
    "                _, _ = train(train_x, train_y,\n",
    "                             n_devices=n_devices, output_device=output_device,\n",
    "                             checkpoint_size=checkpoint_size,\n",
    "                             preconditioner_size=preconditioner_size, n_training_iter=1)\n",
    "\n",
    "                # when successful, break out of for-loop and jump to finally block\n",
    "                break\n",
    "            except RuntimeError as e:\n",
    "                print('RuntimeError: {}'.format(e))\n",
    "            except AttributeError as e:\n",
    "                print('AttributeError: {}'.format(e))\n",
    "            finally:\n",
    "                # handle CUDA OOM error\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        return checkpoint_size\n",
    "\n",
    "    # Set a large enough preconditioner size to reduce the number of CG iterations run\n",
    "    preconditioner_size = 100\n",
    "    checkpoint_size = find_best_gpu_setting(train_x, train_y,\n",
    "                                            n_devices=n_devices,\n",
    "                                            output_device=output_device,\n",
    "                                            preconditioner_size=preconditioner_size)\n",
    "    \n",
    "    \n",
    "    model, likelihood = train(train_x, train_y,\n",
    "                              n_devices=n_devices, output_device=output_device,\n",
    "                              checkpoint_size=checkpoint_size,\n",
    "                              preconditioner_size=preconditioner_size,\n",
    "                              n_training_iter=100)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(),\\\n",
    "         gpytorch.settings.fast_pred_var(),\\\n",
    "         gpytorch.beta_features.checkpoint_kernel(checkpoint_size):        \n",
    "        y_preds = likelihood(model(test_x))\n",
    "        y_mean=y_preds.mean\n",
    "    \n",
    "    \n",
    "    mae=mean_absolute_error(test_y.cpu(),y_mean.cpu())\n",
    "    MAE+=mae\n",
    "    error=(test_y.cpu()-y_mean.cpu()).numpy()\n",
    "    error_vector=np.append(error_vector,error)\n",
    "    me=np.mean(error)\n",
    "    ME+=me\n",
    "    std=np.std(error,ddof=1)\n",
    "    STD+=std*(test_x.cpu().shape[0]-1)\n",
    "    print(patient_list[id],'MAE: ', format(mae,'.3f'),'ME',format(me,'.3f'),'STD: ',format(std,'.3f'))\n",
    "\n",
    "\n",
    "prediction_error={'SBP': np.array(data['SBP']),\n",
    "                 'prediction error': error_vector}\n",
    "prediction_error=pd.DataFrame(prediction_error)\n",
    "prediction_error.to_csv('SBP GPR.csv')###\n",
    "plt.title(\"SBP of GPR.csv\") ###\n",
    "plt.xlabel(\"true value of SBP\") #\n",
    "plt.ylabel(\"prediction error\") \n",
    "plt.scatter(np.array(data['SBP']),error_vector.transpose(),s=0.2) #  \n",
    "plt.show()\n",
    "print('average MAE: ', format(MAE/L,'.3f'),'ME: ', format(ME/L,'.3f'), 'STD: ', format(STD/(data.shape[0]-L),'.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccc50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
