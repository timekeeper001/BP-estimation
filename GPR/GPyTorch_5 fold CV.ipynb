{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c99a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from LBFGS import FullBatchLBFGS\n",
    "from sklearn.model_selection import GroupKFold\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from TCA import kernel,TCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6aeaff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning to run on 2 GPUs.\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('QingData_sbp.csv')\n",
    "#data = pd.read_csv('data_20p_1h_21f.csv')\n",
    "#data=data.sample(frac=0.5,random_state=1)\n",
    "patient_list=data['patient_id'].unique()\n",
    "#print(patient_list)\n",
    "feature=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "#feature=['HrRR','HrSS','AmFS','AmSN','AmFN','AmFN_FS','AmFN_SN','TmFN','TmNF','TmFN_NF','Tm_FS','Tm_SF','Tm_SN','Tm_FQ','PAT_S','PAT_F','PAT_Q','ArFS','ArSN','ArNF','ArNF_FN']\n",
    "n_devices = 2#torch.cuda.device_count()\n",
    "print('Planning to run on {} GPUs.'.format(n_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3749c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: [ 0  9 15 16 21 27 29 30 37 44 50 53 61]\n"
     ]
    }
   ],
   "source": [
    "MAE=0\n",
    "ME=0\n",
    "STD=0\n",
    "F=5\n",
    "X=data[feature]#定义源域训练集\n",
    "y=data['SBP']\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "groups=np.array(data['patient_id'])\n",
    "gkf=GroupKFold(n_splits=F)\n",
    "i=0\n",
    "for train_index , test_index in gkf.split(X, y, groups):\n",
    "    i+=1\n",
    "    #print(\"TRAIN:\", np.unique(groups[train_index]))\n",
    "    print(\"TEST:\", np.unique(groups[test_index]))\n",
    "    train_x,train_y=X[train_index],y.iloc[train_index]\n",
    "    test_x, test_y = X[test_index],y.iloc[test_index]\n",
    "#     tca=TCA(kernel_type='linear', dim=5, lamb=1, gamma=1)\n",
    "#     train_x, test_x=tca.fit(train_x, test_x)\n",
    "    \n",
    "    train_x=torch.from_numpy( np.array(train_x) )\n",
    "    train_y=torch.from_numpy( np.array(train_y) )\n",
    "    test_x=torch.from_numpy( np.array(test_x) )\n",
    "    test_y=torch.from_numpy( np.array(test_y))\n",
    "    \n",
    "    output_device = torch.device('cuda:0')\n",
    "    train_x, train_y = train_x.to(output_device), train_y.to(output_device)\n",
    "    test_x, test_y = test_x.to(output_device), test_y.to(output_device)\n",
    "    \n",
    "    class ExactGPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, n_devices):\n",
    "            super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            base_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "\n",
    "            self.covar_module = gpytorch.kernels.MultiDeviceKernel(\n",
    "                base_covar_module, device_ids=range(n_devices),\n",
    "                output_device=output_device\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def train(train_x,\n",
    "              train_y,\n",
    "              n_devices,\n",
    "              output_device,\n",
    "              checkpoint_size,\n",
    "              preconditioner_size,\n",
    "              n_training_iter,\n",
    "    ):\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(output_device)\n",
    "        model = ExactGPModel(train_x, train_y, likelihood, n_devices).to(output_device)\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = FullBatchLBFGS(model.parameters(), lr=0.1)\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "        with gpytorch.beta_features.checkpoint_kernel(checkpoint_size), \\\n",
    "             gpytorch.settings.max_preconditioner_size(preconditioner_size):\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(train_x)\n",
    "                loss = -mll(output, train_y)\n",
    "                return loss\n",
    "\n",
    "            loss = closure()\n",
    "            loss.backward()\n",
    "\n",
    "            for i in range(n_training_iter):\n",
    "                options = {'closure': closure, 'current_loss': loss, 'max_ls': 10}\n",
    "                loss, _, _, _, _, _, _, fail = optimizer.step(options)\n",
    "\n",
    "#                 print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "#                     i + 1, n_training_iter, loss.item(),\n",
    "#                     model.covar_module.module.base_kernel.lengthscale.item(),\n",
    "#                     model.likelihood.noise.item()\n",
    "#                 ))\n",
    "\n",
    "                if fail:\n",
    "                    print('Convergence reached!')\n",
    "                    break\n",
    "\n",
    "        print(f\"Finished training on {train_x.size(0)} data points using {n_devices} GPUs.\")\n",
    "        return model, likelihood\n",
    "    \n",
    "    def find_best_gpu_setting(train_x,\n",
    "                              train_y,\n",
    "                              n_devices,\n",
    "                              output_device,\n",
    "                              preconditioner_size\n",
    "    ):\n",
    "        N = train_x.size(0)\n",
    "\n",
    "        # Find the optimum partition/checkpoint size by decreasing in powers of 2\n",
    "        # Start with no partitioning (size = 0)\n",
    "        settings = [0] + [int(n) for n in np.ceil(N / 2**np.arange(1, np.floor(np.log2(N))))]\n",
    "\n",
    "        for checkpoint_size in settings:\n",
    "            print('Number of devices: {} -- Kernel partition size: {}'.format(n_devices, checkpoint_size))\n",
    "            try:\n",
    "                # Try a full forward and backward pass with this setting to check memory usage\n",
    "                _, _ = train(train_x, train_y,\n",
    "                             n_devices=n_devices, output_device=output_device,\n",
    "                             checkpoint_size=checkpoint_size,\n",
    "                             preconditioner_size=preconditioner_size, n_training_iter=1)\n",
    "\n",
    "                # when successful, break out of for-loop and jump to finally block\n",
    "                break\n",
    "            except RuntimeError as e:\n",
    "                print('RuntimeError: {}'.format(e))\n",
    "            except AttributeError as e:\n",
    "                print('AttributeError: {}'.format(e))\n",
    "            finally:\n",
    "                # handle CUDA OOM error\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        return checkpoint_size\n",
    "\n",
    "    # Set a large enough preconditioner size to reduce the number of CG iterations run\n",
    "    preconditioner_size = 100\n",
    "    checkpoint_size = find_best_gpu_setting(train_x, train_y,\n",
    "                                            n_devices=n_devices,\n",
    "                                            output_device=output_device,\n",
    "                                            preconditioner_size=preconditioner_size)\n",
    "    \n",
    "    \n",
    "    model, likelihood = train(train_x, train_y,\n",
    "                              n_devices=n_devices, output_device=output_device,\n",
    "                              checkpoint_size=checkpoint_size,\n",
    "                              preconditioner_size=preconditioner_size,\n",
    "                              n_training_iter=100)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(),\\\n",
    "         gpytorch.settings.fast_pred_var(),\\\n",
    "         gpytorch.beta_features.checkpoint_kernel(checkpoint_size):        \n",
    "        y_preds = likelihood(model(test_x))\n",
    "        y_mean=y_preds.mean\n",
    "    \n",
    "    \n",
    "    prediction_data={'ID':groups[test_index],\n",
    "                     '真实值': test_y.cpu(),\n",
    "                     '预测值': y_mean.cpu()}\n",
    "    prediction_data=pd.DataFrame(prediction_data) \n",
    "    prediction_data.to_csv('fold '+ '%d' %i +'.csv')\n",
    "        \n",
    "    mae=mean_absolute_error(test_y.cpu(),y_mean.cpu())\n",
    "    MAE+=mae\n",
    "    error=(test_y.cpu()-y_mean.cpu()).numpy()\n",
    "    me=np.mean(error)\n",
    "    ME+=me\n",
    "    std=np.std(error,ddof=1)\n",
    "    STD+=std*(test_x.cpu().shape[0]-1)\n",
    "    print('Validation on fold',i, 'MAE: ', format(mae,'.3f'),'ME',format(me,'.3f'),'STD: ',format(std,'.3f'))\n",
    "    print('  ')\n",
    "\n",
    "print('average MAE: ', format(MAE/F,'.3f'),'ME: ', format(ME/F,'.3f'), 'STD: ', format(STD/(data.shape[0]-F),'.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d88d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38095042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
